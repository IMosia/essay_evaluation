{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:13.044624Z",
     "iopub.status.busy": "2025-06-20T10:23:13.044297Z",
     "iopub.status.idle": "2025-06-20T10:23:16.830539Z",
     "shell.execute_reply": "2025-06-20T10:23:16.829367Z",
     "shell.execute_reply.started": "2025-06-20T10:23:13.044601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Libraries related to ML  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:16.833811Z",
     "iopub.status.busy": "2025-06-20T10:23:16.832661Z",
     "iopub.status.idle": "2025-06-20T10:23:16.840635Z",
     "shell.execute_reply": "2025-06-20T10:23:16.839607Z",
     "shell.execute_reply.started": "2025-06-20T10:23:16.833777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n",
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\n",
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:16.841803Z",
     "iopub.status.busy": "2025-06-20T10:23:16.841561Z",
     "iopub.status.idle": "2025-06-20T10:23:17.316898Z",
     "shell.execute_reply": "2025-06-20T10:23:17.316022Z",
     "shell.execute_reply.started": "2025-06-20T10:23:16.841781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (17307, 3)\n",
      "Test Data Shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\n",
    "\n",
    "print(\"Train Data Shape:\", train_df.shape)\n",
    "print(\"Test Data Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:17.318929Z",
     "iopub.status.busy": "2025-06-20T10:23:17.318653Z",
     "iopub.status.idle": "2025-06-20T10:23:17.325723Z",
     "shell.execute_reply": "2025-06-20T10:23:17.324773Z",
     "shell.execute_reply.started": "2025-06-20T10:23:17.318901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  essay_id                                          full_text\n",
      "0  000d118  Many people have car where they live. The thin...\n",
      "1  000fe60  I am a scientist at NASA that is discussing th...\n",
      "2  001ab80  People always wish they had the same technolog...\n"
     ]
    }
   ],
   "source": [
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:17.326984Z",
     "iopub.status.busy": "2025-06-20T10:23:17.326680Z",
     "iopub.status.idle": "2025-06-20T10:23:58.244292Z",
     "shell.execute_reply": "2025-06-20T10:23:58.243445Z",
     "shell.execute_reply.started": "2025-06-20T10:23:17.326957Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Text Data: 100%|██████████| 17307/17307 [00:40<00:00, 423.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 s, sys: 871 ms, total: 40.9 s\n",
      "Wall time: 40.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Function to clean the text data by:\n",
    "        - Lowercasing the text\n",
    "        - Removing URLs \n",
    "        - Removing special characters and punctuation\n",
    "        - Tokenizing the text\n",
    "        - Removing stopwords\n",
    "        - Lemmatizing the tokens\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "tqdm.pandas(desc=\"Cleaning Text Data\")\n",
    "train_df['cleaned_text'] = train_df['full_text'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:23:58.245552Z",
     "iopub.status.busy": "2025-06-20T10:23:58.245219Z",
     "iopub.status.idle": "2025-06-20T10:24:00.801014Z",
     "shell.execute_reply": "2025-06-20T10:24:00.800186Z",
     "shell.execute_reply.started": "2025-06-20T10:23:58.245525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def vectorize_text(train, column='cleaned_text', num_features=1000):\n",
    "    \"\"\"\n",
    "    To apply TF-IDF vectorization to the text data.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=num_features)\n",
    "    X_train_vec = vectorizer.fit_transform(train[column])\n",
    "    return X_train_vec,vectorizer\n",
    "\n",
    "set_of_tf_idf_features = {}\n",
    "\n",
    "num_features=3000\n",
    "X_train_vec,  vectorizer = vectorize_text(train_df, num_features=num_features)\n",
    "set_of_tf_idf_features[3000] = {\n",
    "    'X_train': X_train_vec,\n",
    "    'vectorizer': vectorizer\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-20T10:24:00.802132Z",
     "iopub.status.busy": "2025-06-20T10:24:00.801874Z",
     "iopub.status.idle": "2025-06-20T10:24:31.902312Z",
     "shell.execute_reply": "2025-06-20T10:24:31.901466Z",
     "shell.execute_reply.started": "2025-06-20T10:24:00.802113Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 131 ms, total: 1min 1s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# best model LightGBM Regressor\tTF-IDF 3000 features\n",
    "test_df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\n",
    "test_df['cleaned_text'] = test_df['full_text'].apply(clean_text)\n",
    "X_test_vec = set_of_tf_idf_features[3000]['vectorizer'].transform(test_df['cleaned_text'])\n",
    "\n",
    "# retraining the best model on the full training set\n",
    "parameters = {'num_leaves': 12\n",
    "              , 'max_depth': -1\n",
    "              , 'learning_rate': 0.1\n",
    "              , 'n_estimators': 500}\n",
    "final_model = LGBMRegressor(**parameters\n",
    "                            , class_weight='balanced'\n",
    "                            , objective='regression'\n",
    "                            , random_state=42\n",
    "                            , verbose=-1)\n",
    "\n",
    "final_model.fit(X_train_vec, train_df['score'])\n",
    "\n",
    "# Making predictions on the test set\n",
    "test_predictions = final_model.predict(X_test_vec)\n",
    "test_predictions = np.round(test_predictions).astype(int)\n",
    "test_predictions = np.clip(test_predictions, 1, 6)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'essay_id': test_df['essay_id'],\n",
    "    'score': test_predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
